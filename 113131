# crawler.py

import os
import re
import time
import nltk
from nltk.stem.snowball import RussianStemmer
from nltk.corpus import stopwords
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from whoosh.index import open_dir, create_in
from whoosh.fields import Schema, TEXT, ID, STORED

# Ensure NLTK data is downloaded
nltk.download('punkt')
nltk.download('stopwords')


class WebCrawler:
    def __init__(self, root_dirs, index_dir='indexdir'):
        self.root_dirs = root_dirs
        self.visited_files = set()
        self.filepaths_to_visit = []
        self.stemmer = RussianStemmer()
        self.stop_words = set(stopwords.words('russian'))
        self.index_dir = index_dir

        if not os.path.exists(index_dir):
            os.mkdir(index_dir)

        if not os.path.exists(index_dir) or not os.listdir(index_dir):
            self.schema = Schema(
                url=ID(stored=True, unique=True),
                title=TEXT(stored=True),
                content=TEXT,
                description=TEXT(stored=True),
                logo=STORED
            )
            self.ix = create_in(index_dir, self.schema)
            print("Created new Whoosh index.")
        else:
            self.ix = open_dir(index_dir)
            print("Opened existing Whoosh index.")

    def clean_text(self, text):
        tokens = nltk.word_tokenize(text.lower(), language='russian')
        tokens = [re.sub(r'\W+', '', token) for token in tokens]
        tokens = [token for token in tokens if token and token not in self.stop_words]
        tokens = [self.stemmer.stem(token) for token in tokens]
        return ' '.join(tokens)

    def extract_description(self, text):
        description = text.strip().replace('\n', ' ')
        if len(description) > 200:
            description = description[:200] + '...'
        return description

    def extract_logo(self, soup, base_path):
        # Извлекаем путь к логотипу из тегов <img class="site-logo" src="...">
        logo_tag = soup.find("img", {"class": "site-logo"})
        if logo_tag and 'src' in logo_tag.attrs:
            logo_relative = logo_tag['src'].lstrip('/')
            logo_path = os.path.join('search', logo_relative)
            if os.path.exists(logo_path):
                return f"smp://search/{logo_relative}"
        # Если логотип не найден, используем дефолтный
        return "smp://search/logos/default_icon.png"

    def crawl(self, max_pages=100):
        page_count = 0
        for root_dir in self.root_dirs:
            for root, _, files in os.walk(root_dir):
                for file in files:
                    if file.lower().endswith(('.html', '.htm')):
                        filepath = os.path.join(root, file)
                        if filepath in self.visited_files:
                            continue
                        self.visited_files.add(filepath)
                        try:
                            with open(filepath, 'r', encoding='utf-8') as f:
                                soup = BeautifulSoup(f, 'html.parser')
                                title = soup.title.string.strip() if soup.title else 'No Title'
                                text = soup.get_text(separator=' ', strip=True)
                                clean_content = self.clean_text(text)
                                description = self.extract_description(text)

                                logo_url = self.extract_logo(soup, filepath)

                                # Prepare SMP resource path
                                relative_path = os.path.relpath(filepath, root_dir).replace(os.sep, '/')
                                resource_path = f"smp://{os.path.basename(root_dir)}/{relative_path}"

                                # Add document to index
                                writer = self.ix.writer()
                                writer.update_document(
                                    url=resource_path,
                                    title=title,
                                    content=clean_content,
                                    description=description,
                                    logo=logo_url
                                )
                                writer.commit()
                                page_count += 1
                                print(f"Indexed: {resource_path}")

                                if page_count >= max_pages:
                                    print(f"Reached max_pages limit: {max_pages}")
                                    return
                                time.sleep(0.1)  # Небольшая задержка для избежания высокой нагрузки
                        except Exception as e:
                            print(f"Failed to index file {filepath}: {e}")
                            continue
        print(f"Crawling completed. Total pages indexed: {page_count}")


if __name__ == "__main__":
    root_dirs = [
        'search',
        'sites'
    ]
    crawler = WebCrawler(root_dirs, index_dir='indexdir')
    crawler.crawl(max_pages=100)
