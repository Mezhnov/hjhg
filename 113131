# crawler.py

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time
import nltk
from nltk.stem.snowball import RussianStemmer
from nltk.corpus import stopwords
from whoosh.index import open_dir, create_in
from whoosh.fields import Schema, TEXT, ID, STORED
from whoosh.qparser import QueryParser
import os

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')

class WebCrawler:
    def __init__(self, start_urls, allowed_domains, index_dir='indexdir'):
        self.start_urls = start_urls
        self.allowed_domains = allowed_domains
        self.visited_urls = set()
        self.urls_to_visit = start_urls[:]
        self.stemmer = RussianStemmer()
        self.stop_words = set(stopwords.words('russian'))
        self.index_dir = index_dir
        if not os.path.exists(index_dir):
            os.mkdir(index_dir)
        if not os.path.exists(index_dir) or not os.listdir(index_dir):
            self.schema = Schema(
                url=ID(stored=True, unique=True),
                title=TEXT(stored=True),
                content=TEXT,
                description=TEXT(stored=True),
                logo=STORED
            )
            self.ix = create_in(index_dir, self.schema)
            print("Created new Whoosh index.")
        else:
            self.ix = open_dir(index_dir)
            print("Opened existing Whoosh index.")

    def is_allowed(self, url):
        parsed_url = urlparse(url)
        return any(allowed_domain in parsed_url.netloc for allowed_domain in self.allowed_domains)

    def clean_text(self, text):
        tokens = nltk.word_tokenize(text.lower(), language='russian')
        tokens = [re.sub(r'\W+', '', token) for token in tokens]
        tokens = [token for token in tokens if token and token not in self.stop_words]
        tokens = [self.stemmer.stem(token) for token in tokens]
        return ' '.join(tokens)

    def extract_description(self, text):
        description = text.strip().replace('\n', ' ')
        if len(description) > 200:
            description = description[:200] + '...'
        return description

    def extract_logo(self, soup, base_url):
        icon_link = soup.find("link", rel=lambda r: r and "icon" in r.lower())
        if icon_link and 'href' in icon_link.attrs:
            logo_url = urljoin(base_url, icon_link['href'])
            return logo_url
        return None

    def save_logo(self, logo_url, url):
        try:
            response = requests.get(logo_url, stream=True, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
            if response.status_code == 200:
                parsed_url = urlparse(url)
                domain = parsed_url.netloc.replace(':', '_')
                ext = os.path.splitext(logo_url)[1] if os.path.splitext(logo_url)[1] else '.ico'
                logo_filename = f"{domain}{ext}"
                logos_dir = os.path.join('static', 'logos')
                if not os.path.exists(logos_dir):
                    os.makedirs(logos_dir)
                logo_path = os.path.join(logos_dir, logo_filename)
                with open(logo_path, 'wb') as f:
                    for chunk in response.iter_content(1024):
                        f.write(chunk)
                return os.path.join('logos', logo_filename)
        except requests.RequestException as e:
            print(f"Failed to download logo: {e}")
        return None

    def crawl(self, max_pages=100):
        page_count = 0
        while self.urls_to_visit and page_count < max_pages:
            url = self.urls_to_visit.pop(0)
            if url in self.visited_urls:
                continue
            self.visited_urls.add(url)
            if not self.is_allowed(url):
                continue
            try:
                response = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    title_tag = soup.find('title')
                    title = title_tag.get_text().strip() if title_tag else url
                    text = soup.get_text(separator=' ', strip=True)
                    clean_content = self.clean_text(text)
                    description = self.extract_description(text)

                    logo_url = self.extract_logo(soup, url)
                    logo_path = None
                    if logo_url:
                        logo_path = self.save_logo(logo_url, url)

                    # Update Whoosh index
                    writer = self.ix.writer()
                    writer.update_document(
                        url=url,
                        title=title,
                        content=clean_content,
                        description=description,
                        logo=logo_path
                    )
                    writer.commit()
                    page_count += 1
                    print(f"Indexed: {url}")

                    # Extract and enqueue all found links
                    for link in soup.find_all('a', href=True):
                        absolute_link = urljoin(url, link['href'])
                        if absolute_link not in self.visited_urls and self.is_allowed(absolute_link):
                            self.urls_to_visit.append(absolute_link)
                    time.sleep(1)  # Respectful crawling
            except requests.RequestException as e:
                print(f"Request failed: {e}")
                continue
            except Exception as e:
                print(f"An error occurred: {e}")
                continue

        print(f"Crawling completed. Total pages indexed: {page_count}")

if __name__ == "__main__":
    start_urls = [
        'smp://garik2004.pythonanywhere.com/'  # Example SMP URL
    ]
    allowed_domains = [
        'garik2004.pythonanywhere.com'
    ]
    crawler = WebCrawler(start_urls, allowed_domains, index_dir=INDEX_DIR)
    crawler.crawl(max_pages=50)
